{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing packages\n",
    "import nltk\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from distutils.version import LooseVersion\n",
    "import warnings\n",
    "from tensorflow.python.layers.core import Dense\n",
    "import pickle\n",
    "import os\n",
    "import copy\n",
    "import re\n",
    "import unicodedata\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "# https://github.com/deep-diver/EN-FR-MLT-tensorflow/blob/master/dlnd_language_translationv2.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets see if we can preprocess it like the example\n",
    "f=open('PrideandPred.txt', encoding = 'utf8')\n",
    "st2 = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "raw=f.read()\n",
    "raw = raw.replace('\\ufeff','').replace('\\n',' ').replace('  ',' ').replace('_','')\n",
    "for i in range(0,100):\n",
    "    raw = raw.replace('Chapter '+str(i)+' ','')\n",
    "raw = re.sub('([.,!?()])', r' \\1 ', raw)\n",
    "raw = re.sub('\\s{2,}', ' ', raw)\n",
    "raw = raw.lower()\n",
    "#raw = '\\n'.join(st2.tokenize(raw, realign_boundaries=True))\n",
    "raw = st2.tokenize(raw, realign_boundaries=True)\n",
    "print(*raw, sep='\\n\\n')\n",
    "# Lets hold off on Pride and Prejudice for now, using the example text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137861\n",
      "227\n"
     ]
    }
   ],
   "source": [
    "f = open('small_vocab_en.txt')\n",
    "raw = f.read()\n",
    "print(len(raw.split('\\n')))\n",
    "print(len(set(raw.split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12797\n",
      "Thank you .\n",
      "-- You remember .\n",
      "They're not going to listen to me . You either . Are you kidding ?\n",
      "It's only temporary . On a freelance basis .\n",
      "Rusty's dead . That's Ajax\n",
      "That's what I was hoping you'd tell me .\n",
      "I'll call you Mark when you catch the killer , Detective .\n",
      "Precisely , Maria . Tonight you have the same privilege that comes on rare occasions to the chief executive of some state or nation . . . the privilege of restoring life , by one tender act of mercy , to a doomed fellow creature .\n",
      "I can feel air .\n",
      "Uh , yoo-hoo . Excuse me ! Sorry to interrupt , but I got some big news .\n",
      "They don't have substitute teachers where you go to school ?\n",
      "It probably will . In fact , I'd go so far as to say it almost certainly will , in time . Why should I settle for that ?\n",
      "He says it's a box .\n",
      "I'm thinking about producing .\n",
      "There is a motel downtown , near the Machine Shop . . . the Suncrest . Room 138 .\n",
      "Working on it now , sir .\n",
      "There is a 20th Century possibility .\n",
      "What ? What is it ?\n",
      "Ben is ve\n"
     ]
    }
   ],
   "source": [
    "# Okay... lets try movie lines instead\n",
    "f=open('movie_lines.txt', encoding = 'utf-8', errors='replace')\n",
    "raw=f.read()\n",
    "raw = raw.split('\\n')\n",
    "for i in ['.',',','!','?',';',':','\"']:\n",
    "    raw = [x.replace(i,' '+i+' ') for x in raw]\n",
    "raw = [x.replace('. . .','...') for x in raw]\n",
    "raw = [' '.join(x.split()) for x in raw]\n",
    "raw = [x.split('+++$+++')[-1] for x in raw]\n",
    "#raw = [x.replace(' . ', ' . \\n') for x in raw]\n",
    "raw = [x.lstrip() for x in raw]\n",
    "raw = '\\n'.join([x for x in raw if len(x)>10])\n",
    "raw = unicodedata.normalize('NFKD', raw)\n",
    "raw = '\\n'.join(random.sample(raw.split('\\n'),10000))\n",
    "print(len(set(raw.split())))\n",
    "print(raw[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thank you .\n",
      "-- You remember .\n",
      "They're not going to listen to me . You either . Are you kidding ?\n",
      "It's only temporary . On a freelance basis .\n",
      "Rusty's dead . That's Ajax\n",
      "That's what I was hoping you'd tell me .\n",
      "I'll call you Mark when you catch the killer , Detective .\n",
      "Precisely , Maria . Tonight you have the same privilege that comes on rare occasions to the chief executive of some state or nation . . . the privilege of restoring life , by one tender act of mercy , to a doomed fellow creature .\n",
      "I can feel air .\n",
      "Uh , yoo-hoo . Excuse me ! Sorry to interrupt , but I got some big news .\n",
      "They don't have substitute teachers where you go to school ?\n",
      "It probably will . In fact , I'd go so far as to say it almost certainly will , in time . Why should I settle for that ?\n",
      "He says it's a box .\n",
      "I'm thinking about producing .\n",
      "There is a motel downtown , near the Machine Shop . . . the Suncrest . Room 138 .\n",
      "Working on it now , sir .\n",
      "There is a 20th Century possibility .\n",
      "What ? What is it ?\n",
      "Ben is ve\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "# Saving as a text file\n",
    "file = open('ProcessedScript.txt', 'w', encoding = 'utf-8', errors = 'replace') \n",
    "file.write(raw)\n",
    "file.close() \n",
    "\n",
    "# Can we read it back in\n",
    "raw = open('ProcessedScript.txt', 'r', encoding = 'utf-8', errors = 'replace')\n",
    "raw = raw.read()\n",
    "print(raw[0:1000])\n",
    "print(len(raw.split('\\n')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.12.0\n",
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# Check TensorFlow Version\n",
    "assert LooseVersion(tf.__version__) >= LooseVersion('1.1'), 'Please use TensorFlow version 1.1 or newer'\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('No GPU found. Please use a GPU to train your neural network.')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing and Preparing data\n",
    "def load_data(path, vocab_size):\n",
    "    input_file = os.path.join(path)\n",
    "    with open(input_file, 'r', encoding='utf-8', errors='replace') as f:\n",
    "        data = f.read()\n",
    "        #data = data.replace('\\ufeff','').replace('\\n',' ').replace('  ',' ').replace('_','')\n",
    "        #for i in range(0,100):\n",
    "        #    data = data.replace('Chapter '+str(i)+' ','')\n",
    "        #data = ' '.join(nltk.tokenize.sent_tokenize(data)[0:20000])\n",
    "    if vocab_size != None:\n",
    "        top_n = [x[0] for x in Counter(data.split()).most_common(vocab_size)]\n",
    "        data = ' '.join([x if x in top_n else '<UNK>' for x in data.split()])\n",
    "        data = '\\n'.join([x for x in data.split('\\n') if len(set(x.split()))>1])\n",
    "\n",
    "    return data\n",
    "\n",
    "CODES = {'<PAD>': 0, '<EOS>': 1, '<UNK>': 2, '<GO>': 3 }\n",
    "\n",
    "def create_lookup_tables(text):\n",
    "    # make a list of unique words\n",
    "    vocab = set(text.split())\n",
    "    #vocab = set(nltk.tokenize.word_tokenize(text))\n",
    "\n",
    "    # (1)\n",
    "    # starts with the special tokens\n",
    "    vocab_to_int = copy.copy(CODES)\n",
    "\n",
    "    # the index (v_i) will starts from 4 (the 2nd arg in enumerate() specifies the starting index)\n",
    "    # since vocab_to_int already contains special tokens\n",
    "    for v_i, v in enumerate(vocab, len(CODES)):\n",
    "        vocab_to_int[v] = v_i\n",
    "\n",
    "    # (2)\n",
    "    int_to_vocab = {v_i: v for v, v_i in vocab_to_int.items()}\n",
    "\n",
    "    return vocab_to_int, int_to_vocab\n",
    "\n",
    "def text_to_ids(source_text, target_text, source_vocab_to_int, target_vocab_to_int):\n",
    "    \"\"\"\n",
    "        1st, 2nd args: raw string text to be converted\n",
    "        3rd, 4th args: lookup tables for 1st and 2nd args respectively\n",
    "    \n",
    "        return: A tuple of lists (source_id_text, target_id_text) converted\n",
    "    \"\"\"\n",
    "    # empty list of converted sentences\n",
    "    source_text_id = []\n",
    "    target_text_id = []\n",
    "    \n",
    "    # make a list of sentences (extraction)\n",
    "    source_sentences = source_text.split(\"\\n\")\n",
    "    target_sentences = target_text.split(\"\\n\")\n",
    "    #source_sentences = nltk.tokenize.sent_tokenize(source_text)\n",
    "    #target_sentences = nltk.tokenize.sent_tokenize(target_text)\n",
    "    \n",
    "    max_source_sentence_length = max([len(sentence.split(\" \")) for sentence in source_sentences])\n",
    "    max_target_sentence_length = max([len(sentence.split(\" \")) for sentence in target_sentences])\n",
    "    \n",
    "    # iterating through each sentences (# of sentences in source&target is the same)\n",
    "    for i in range(len(source_sentences)):\n",
    "        # extract sentences one by one\n",
    "        source_sentence = source_sentences[i]\n",
    "        target_sentence = target_sentences[i]\n",
    "        \n",
    "        # Putting whitespace around punctuation\n",
    "        #source_sentence = re.sub('([.,!?()])', r' \\1 ', source_sentence)\n",
    "        #source_sentence = re.sub('\\s{2,}', ' ', source_sentence)\n",
    "        #target_sentence = re.sub('([.,!?()])', r' \\1 ', target_sentence)\n",
    "        #target_sentence = re.sub('\\s{2,}', ' ', target_sentence)\n",
    "        \n",
    "        # make a list of tokens/words (extraction) from the chosen sentence\n",
    "        source_tokens = source_sentence.split(\" \")\n",
    "        target_tokens = target_sentence.split(\" \")\n",
    "        #source_tokens = nltk.tokenize.word_tokenize(source_sentence)\n",
    "        #target_tokens = nltk.tokenize.word_tokenize(target_sentence)\n",
    "        \n",
    "        # empty list of converted words to index in the chosen sentence\n",
    "        source_token_id = []\n",
    "        target_token_id = []\n",
    "        \n",
    "        for index, token in enumerate(source_tokens):\n",
    "            if (token != \"\"):\n",
    "                source_token_id.append(source_vocab_to_int[token])\n",
    "        \n",
    "        for index, token in enumerate(target_tokens):\n",
    "            if (token != \"\"):\n",
    "                target_token_id.append(target_vocab_to_int[token])\n",
    "                \n",
    "        # put <EOS> token at the end of the chosen target sentence\n",
    "        # this token suggests when to stop creating a sequence\n",
    "        target_token_id.append(target_vocab_to_int['<EOS>'])\n",
    "            \n",
    "        # add each converted sentences in the final list\n",
    "        source_text_id.append(source_token_id)\n",
    "        target_text_id.append(target_token_id)\n",
    "    \n",
    "    return source_text_id, target_text_id\n",
    "\n",
    "\n",
    "def preprocess_and_save_data(source_path, target_path, text_to_ids, max_vocab=None):\n",
    "    # Preprocess\n",
    "    \n",
    "    # load original data (English, French)\n",
    "    source_text = load_data(source_path, max_vocab)\n",
    "    target_text = load_data(target_path, max_vocab)\n",
    "\n",
    "    # to the lower case\n",
    "    source_text = source_text.lower()\n",
    "    target_text = target_text.lower()\n",
    "\n",
    "    # create lookup tables for data\n",
    "    source_vocab_to_int, source_int_to_vocab = create_lookup_tables(source_text)\n",
    "    target_vocab_to_int, target_int_to_vocab = create_lookup_tables(target_text)\n",
    "\n",
    "    # create list of sentences whose words are represented in index\n",
    "    source_text, target_text = text_to_ids(source_text, target_text, source_vocab_to_int, target_vocab_to_int)\n",
    "\n",
    "    # Save data for later use\n",
    "    pickle.dump((\n",
    "        (source_text, target_text),\n",
    "        (source_vocab_to_int, target_vocab_to_int),\n",
    "        (source_int_to_vocab, target_int_to_vocab)), open('preprocess.p', 'wb'))\n",
    "    \n",
    "source_path = 'small_vocab_en.txt'\n",
    "target_path = 'small_vocab_en.txt'\n",
    "preprocess_and_save_data(source_path, target_path, text_to_ids, max_vocab=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_preprocess():\n",
    "    with open('preprocess.p', mode='rb') as in_file:\n",
    "        return pickle.load(in_file)\n",
    "    \n",
    "(source_int_text, target_int_text), (source_vocab_to_int, target_vocab_to_int), _ = load_preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enc_dec_model_inputs():\n",
    "    inputs = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name='targets') \n",
    "    \n",
    "    target_sequence_length = tf.placeholder(tf.int32, [None], name='target_sequence_length')\n",
    "    max_target_len = tf.reduce_max(target_sequence_length)    \n",
    "    \n",
    "    return inputs, targets, target_sequence_length, max_target_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparam_inputs():\n",
    "    lr_rate = tf.placeholder(tf.float32, name='lr_rate')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    return lr_rate, keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_decoder_input(target_data, target_vocab_to_int, batch_size):\n",
    "    \"\"\"\n",
    "    Preprocess target data for encoding\n",
    "    :return: Preprocessed target data\n",
    "    \"\"\"\n",
    "    # get '<GO>' id\n",
    "    go_id = target_vocab_to_int['<GO>']\n",
    "    \n",
    "    after_slice = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n",
    "    after_concat = tf.concat( [tf.fill([batch_size, 1], go_id), after_slice], 1)\n",
    "    \n",
    "    return after_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_layer(rnn_inputs, rnn_size, num_layers, keep_prob, \n",
    "                   source_vocab_size, \n",
    "                   encoding_embedding_size):\n",
    "    \"\"\"\n",
    "    :return: tuple (RNN output, RNN state)\n",
    "    \"\"\"\n",
    "    embed = tf.contrib.layers.embed_sequence(rnn_inputs, \n",
    "                                             vocab_size=source_vocab_size, \n",
    "                                             embed_dim=encoding_embedding_size)\n",
    "    \n",
    "    stacked_cells = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.DropoutWrapper(tf.contrib.rnn.LSTMCell(rnn_size), keep_prob) for _ in range(num_layers)])\n",
    "    \n",
    "    outputs, state = tf.nn.dynamic_rnn(stacked_cells, \n",
    "                                       embed, \n",
    "                                       dtype=tf.float32)\n",
    "    return outputs, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoding_layer_train(encoder_state, dec_cell, dec_embed_input, \n",
    "                         target_sequence_length, max_summary_length, \n",
    "                         output_layer, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a training process in decoding layer \n",
    "    :return: BasicDecoderOutput containing training logits and sample_id\n",
    "    \"\"\"\n",
    "    dec_cell = tf.contrib.rnn.DropoutWrapper(dec_cell, \n",
    "                                             output_keep_prob=keep_prob)\n",
    "    \n",
    "    # for only input layer\n",
    "    helper = tf.contrib.seq2seq.TrainingHelper(dec_embed_input, \n",
    "                                               target_sequence_length)\n",
    "    \n",
    "    decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell, \n",
    "                                              helper, \n",
    "                                              encoder_state, \n",
    "                                              output_layer)\n",
    "\n",
    "    # unrolling the decoder layer\n",
    "    outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder, \n",
    "                                                      impute_finished=True, \n",
    "                                                      maximum_iterations=max_summary_length)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoding_layer_infer(encoder_state, dec_cell, dec_embeddings, start_of_sequence_id,\n",
    "                         end_of_sequence_id, max_target_sequence_length,\n",
    "                         vocab_size, output_layer, batch_size, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a inference process in decoding layer \n",
    "    :return: BasicDecoderOutput containing inference logits and sample_id\n",
    "    \"\"\"\n",
    "    dec_cell = tf.contrib.rnn.DropoutWrapper(dec_cell, \n",
    "                                             output_keep_prob=keep_prob)\n",
    "    \n",
    "    helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(dec_embeddings, \n",
    "                                                      tf.fill([batch_size], start_of_sequence_id), \n",
    "                                                      end_of_sequence_id)\n",
    "    \n",
    "    decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell, \n",
    "                                              helper, \n",
    "                                              encoder_state, \n",
    "                                              output_layer)\n",
    "    \n",
    "    outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder, \n",
    "                                                      impute_finished=True, \n",
    "                                                      maximum_iterations=max_target_sequence_length)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoding_layer(dec_input, encoder_state,\n",
    "                   target_sequence_length, max_target_sequence_length,\n",
    "                   rnn_size,\n",
    "                   num_layers, target_vocab_to_int, target_vocab_size,\n",
    "                   batch_size, keep_prob, decoding_embedding_size):\n",
    "    \"\"\"\n",
    "    Create decoding layer\n",
    "    :return: Tuple of (Training BasicDecoderOutput, Inference BasicDecoderOutput)\n",
    "    \"\"\"\n",
    "    target_vocab_size = len(target_vocab_to_int)\n",
    "    dec_embeddings = tf.Variable(tf.random_uniform([target_vocab_size, decoding_embedding_size]))\n",
    "    dec_embed_input = tf.nn.embedding_lookup(dec_embeddings, dec_input)\n",
    "    \n",
    "    cells = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.LSTMCell(rnn_size) for _ in range(num_layers)])\n",
    "    \n",
    "    with tf.variable_scope(\"decode\"):\n",
    "        output_layer = tf.layers.Dense(target_vocab_size)\n",
    "        train_output = decoding_layer_train(encoder_state, \n",
    "                                            cells, \n",
    "                                            dec_embed_input, \n",
    "                                            target_sequence_length, \n",
    "                                            max_target_sequence_length, \n",
    "                                            output_layer, \n",
    "                                            keep_prob)\n",
    "\n",
    "    with tf.variable_scope(\"decode\", reuse=True):\n",
    "        infer_output = decoding_layer_infer(encoder_state, \n",
    "                                            cells, \n",
    "                                            dec_embeddings, \n",
    "                                            target_vocab_to_int['<GO>'], \n",
    "                                            target_vocab_to_int['<EOS>'], \n",
    "                                            max_target_sequence_length, \n",
    "                                            target_vocab_size, \n",
    "                                            output_layer,\n",
    "                                            batch_size,\n",
    "                                            keep_prob)\n",
    "\n",
    "    return (train_output, infer_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq_model(input_data, target_data, keep_prob, batch_size,\n",
    "                  target_sequence_length,\n",
    "                  max_target_sentence_length,\n",
    "                  source_vocab_size, target_vocab_size,\n",
    "                  enc_embedding_size, dec_embedding_size,\n",
    "                  rnn_size, num_layers, target_vocab_to_int):\n",
    "    \"\"\"\n",
    "    Build the Sequence-to-Sequence model\n",
    "    :return: Tuple of (Training BasicDecoderOutput, Inference BasicDecoderOutput)\n",
    "    \"\"\"\n",
    "    enc_outputs, enc_states = encoding_layer(input_data, \n",
    "                                             rnn_size, \n",
    "                                             num_layers, \n",
    "                                             keep_prob, \n",
    "                                             source_vocab_size, \n",
    "                                             enc_embedding_size)\n",
    "    \n",
    "    dec_input = process_decoder_input(target_data, \n",
    "                                      target_vocab_to_int, \n",
    "                                      batch_size)\n",
    "    \n",
    "    train_output, infer_output = decoding_layer(dec_input,\n",
    "                                               enc_states, \n",
    "                                               target_sequence_length, \n",
    "                                               max_target_sentence_length,\n",
    "                                               rnn_size,\n",
    "                                              num_layers,\n",
    "                                              target_vocab_to_int,\n",
    "                                              target_vocab_size,\n",
    "                                              batch_size,\n",
    "                                              keep_prob,\n",
    "                                              dec_embedding_size)\n",
    "    \n",
    "    return train_output, infer_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_step = 300\n",
    "\n",
    "epochs = 5\n",
    "batch_size = 64\n",
    "\n",
    "rnn_size = 128\n",
    "num_layers = 3\n",
    "\n",
    "encoding_embedding_size = 200\n",
    "decoding_embedding_size = 200\n",
    "\n",
    "learning_rate = 0.001\n",
    "keep_probability = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'checkpoints/dev'\n",
    "(source_int_text, target_int_text), (source_vocab_to_int, target_vocab_to_int), _ = load_preprocess()\n",
    "max_target_sentence_length = max([len(sentence) for sentence in source_int_text])\n",
    "\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    input_data, targets, target_sequence_length, max_target_sequence_length = enc_dec_model_inputs()\n",
    "    lr, keep_prob = hyperparam_inputs()\n",
    "    \n",
    "    train_logits, inference_logits = seq2seq_model(tf.reverse(input_data, [-1]),\n",
    "                                                   targets,\n",
    "                                                   keep_prob,\n",
    "                                                   batch_size,\n",
    "                                                   target_sequence_length,\n",
    "                                                   max_target_sequence_length,\n",
    "                                                   len(source_vocab_to_int),\n",
    "                                                   len(target_vocab_to_int),\n",
    "                                                   encoding_embedding_size,\n",
    "                                                   decoding_embedding_size,\n",
    "                                                   rnn_size,\n",
    "                                                   num_layers,\n",
    "                                                   target_vocab_to_int)\n",
    "    \n",
    "    training_logits = tf.identity(train_logits.rnn_output, name='logits')\n",
    "    inference_logits = tf.identity(inference_logits.sample_id, name='predictions')\n",
    "\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/sequence_mask\n",
    "    # - Returns a mask tensor representing the first N positions of each cell.\n",
    "    masks = tf.sequence_mask(target_sequence_length, max_target_sequence_length, dtype=tf.float32, name='masks')\n",
    "\n",
    "    with tf.name_scope(\"optimization\"):\n",
    "        # Loss function - weighted softmax cross entropy\n",
    "        cost = tf.contrib.seq2seq.sequence_loss(\n",
    "            training_logits,\n",
    "            targets,\n",
    "            masks)\n",
    "\n",
    "        # Optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "        # Gradient Clipping\n",
    "        gradients = optimizer.compute_gradients(cost)\n",
    "        capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
    "        train_op = optimizer.apply_gradients(capped_gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sentence_batch(sentence_batch, pad_int):\n",
    "    \"\"\"Pad sentences with <PAD> so that each sentence of a batch has the same length\"\"\"\n",
    "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
    "    return [sentence + [pad_int] * (max_sentence - len(sentence)) for sentence in sentence_batch]\n",
    "\n",
    "\n",
    "def get_batches(sources, targets, batch_size, source_pad_int, target_pad_int):\n",
    "    \"\"\"Batch targets, sources, and the lengths of their sentences together\"\"\"\n",
    "    for batch_i in range(0, len(sources)//batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "\n",
    "        # Slice the right amount for the batch\n",
    "        sources_batch = sources[start_i:start_i + batch_size]\n",
    "        targets_batch = targets[start_i:start_i + batch_size]\n",
    "\n",
    "        # Pad\n",
    "        pad_sources_batch = np.array(pad_sentence_batch(sources_batch, source_pad_int))\n",
    "        pad_targets_batch = np.array(pad_sentence_batch(targets_batch, target_pad_int))\n",
    "\n",
    "        # Need the lengths for the _lengths parameters\n",
    "        pad_targets_lengths = []\n",
    "        for target in pad_targets_batch:\n",
    "            pad_targets_lengths.append(len(target))\n",
    "\n",
    "        pad_source_lengths = []\n",
    "        for source in pad_sources_batch:\n",
    "            pad_source_lengths.append(len(source))\n",
    "\n",
    "        yield pad_sources_batch, pad_targets_batch, pad_source_lengths, pad_targets_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done splitting into train and val!\n",
      "Starting seq2seq training...\n",
      "Epoch   1 Batch  300/2154 - Train Accuracy: 0.6011, Validation Accuracy: 0.5859, Loss: 1.6724\n",
      "Epoch   1 Batch  600/2154 - Train Accuracy: 0.6780, Validation Accuracy: 0.6762, Loss: 1.0294\n",
      "Epoch   1 Batch  900/2154 - Train Accuracy: 0.7283, Validation Accuracy: 0.7170, Loss: 0.8420\n",
      "Epoch   1 Batch 1200/2154 - Train Accuracy: 0.7448, Validation Accuracy: 0.7344, Loss: 0.7366\n",
      "Epoch   1 Batch 1500/2154 - Train Accuracy: 0.7517, Validation Accuracy: 0.7465, Loss: 0.6965\n",
      "Epoch   1 Batch 1800/2154 - Train Accuracy: 0.7474, Validation Accuracy: 0.7474, Loss: 0.6946\n",
      "Epoch   1 Batch 2100/2154 - Train Accuracy: 0.7648, Validation Accuracy: 0.7526, Loss: 0.6409\n",
      "Epoch   2 Batch  300/2154 - Train Accuracy: 0.7518, Validation Accuracy: 0.7691, Loss: 0.6751\n",
      "Epoch   2 Batch  600/2154 - Train Accuracy: 0.8177, Validation Accuracy: 0.8012, Loss: 0.5752\n",
      "Epoch   2 Batch  900/2154 - Train Accuracy: 0.8212, Validation Accuracy: 0.8125, Loss: 0.5492\n",
      "Epoch   2 Batch 1200/2154 - Train Accuracy: 0.8194, Validation Accuracy: 0.8168, Loss: 0.5338\n",
      "Epoch   2 Batch 1500/2154 - Train Accuracy: 0.8238, Validation Accuracy: 0.8108, Loss: 0.5150\n",
      "Epoch   2 Batch 1800/2154 - Train Accuracy: 0.8247, Validation Accuracy: 0.8134, Loss: 0.5348\n",
      "Epoch   2 Batch 2100/2154 - Train Accuracy: 0.8325, Validation Accuracy: 0.8273, Loss: 0.4817\n",
      "Epoch   3 Batch  300/2154 - Train Accuracy: 0.8382, Validation Accuracy: 0.8316, Loss: 0.4799\n",
      "Epoch   3 Batch  600/2154 - Train Accuracy: 0.8698, Validation Accuracy: 0.8472, Loss: 0.3843\n",
      "Epoch   3 Batch  900/2154 - Train Accuracy: 0.8620, Validation Accuracy: 0.8542, Loss: 0.3708\n",
      "Epoch   3 Batch 1200/2154 - Train Accuracy: 0.8863, Validation Accuracy: 0.8741, Loss: 0.3153\n",
      "Epoch   3 Batch 1500/2154 - Train Accuracy: 0.8906, Validation Accuracy: 0.8837, Loss: 0.2865\n",
      "Epoch   3 Batch 1800/2154 - Train Accuracy: 0.9062, Validation Accuracy: 0.8993, Loss: 0.2635\n",
      "Epoch   3 Batch 2100/2154 - Train Accuracy: 0.9271, Validation Accuracy: 0.9097, Loss: 0.2350\n",
      "Epoch   4 Batch  300/2154 - Train Accuracy: 0.9228, Validation Accuracy: 0.9184, Loss: 0.2462\n",
      "Epoch   4 Batch  600/2154 - Train Accuracy: 0.9540, Validation Accuracy: 0.9410, Loss: 0.1985\n",
      "Epoch   4 Batch  900/2154 - Train Accuracy: 0.9384, Validation Accuracy: 0.9193, Loss: 0.1866\n",
      "Epoch   4 Batch 1200/2154 - Train Accuracy: 0.9566, Validation Accuracy: 0.9549, Loss: 0.1567\n",
      "Epoch   4 Batch 1500/2154 - Train Accuracy: 0.9627, Validation Accuracy: 0.9427, Loss: 0.1370\n",
      "Epoch   4 Batch 1800/2154 - Train Accuracy: 0.9661, Validation Accuracy: 0.9566, Loss: 0.1270\n",
      "Epoch   4 Batch 2100/2154 - Train Accuracy: 0.9792, Validation Accuracy: 0.9800, Loss: 0.0979\n",
      "Epoch   5 Batch  300/2154 - Train Accuracy: 0.9770, Validation Accuracy: 0.9714, Loss: 0.1173\n",
      "Epoch   5 Batch  600/2154 - Train Accuracy: 0.9826, Validation Accuracy: 0.9809, Loss: 0.0719\n",
      "Epoch   5 Batch  900/2154 - Train Accuracy: 0.9826, Validation Accuracy: 0.9679, Loss: 0.0811\n",
      "Epoch   5 Batch 1200/2154 - Train Accuracy: 0.9861, Validation Accuracy: 0.9844, Loss: 0.0555\n",
      "Epoch   5 Batch 1500/2154 - Train Accuracy: 0.9887, Validation Accuracy: 0.9809, Loss: 0.0450\n",
      "Epoch   5 Batch 1800/2154 - Train Accuracy: 0.9896, Validation Accuracy: 0.9878, Loss: 0.0550\n",
      "Epoch   5 Batch 2100/2154 - Train Accuracy: 0.9939, Validation Accuracy: 0.9887, Loss: 0.0457\n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "def get_accuracy(target, logits):\n",
    "    \"\"\"\n",
    "    Calculate accuracy\n",
    "    \"\"\"\n",
    "    max_seq = max(target.shape[1], logits.shape[1])\n",
    "    if max_seq - target.shape[1]:\n",
    "        target = np.pad(\n",
    "            target,\n",
    "            [(0,0),(0,max_seq - target.shape[1])],\n",
    "            'constant')\n",
    "    if max_seq - logits.shape[1]:\n",
    "        logits = np.pad(\n",
    "            logits,\n",
    "            [(0,0),(0,max_seq - logits.shape[1])],\n",
    "            'constant')\n",
    "\n",
    "    return np.mean(np.equal(target, logits))\n",
    "\n",
    "# Split data to training and validation sets\n",
    "train_source = source_int_text[batch_size:]\n",
    "train_target = target_int_text[batch_size:]\n",
    "valid_source = source_int_text[:batch_size]\n",
    "valid_target = target_int_text[:batch_size]\n",
    "(valid_sources_batch, valid_targets_batch, valid_sources_lengths, valid_targets_lengths ) = next(get_batches(valid_source,\n",
    "                                                                                                             valid_target,\n",
    "                                                                                                             batch_size,\n",
    "                                                                                                             source_vocab_to_int['<PAD>'],\n",
    "                                                                                                             target_vocab_to_int['<PAD>']))                                                                                                  \n",
    "print('Done splitting into train and val!')\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print('Starting seq2seq training...')\n",
    "    for epoch_i in range(epochs):\n",
    "        for batch_i, (source_batch, target_batch, sources_lengths, targets_lengths) in enumerate(\n",
    "                get_batches(train_source, train_target, batch_size,\n",
    "                            source_vocab_to_int['<PAD>'],\n",
    "                            target_vocab_to_int['<PAD>'])):\n",
    "\n",
    "            _, loss = sess.run(\n",
    "                [train_op, cost],\n",
    "                {input_data: source_batch,\n",
    "                 targets: target_batch,\n",
    "                 lr: learning_rate,\n",
    "                 target_sequence_length: targets_lengths,\n",
    "                 keep_prob: keep_probability})\n",
    "\n",
    "\n",
    "            if batch_i % display_step == 0 and batch_i > 0:\n",
    "                batch_train_logits = sess.run(\n",
    "                    inference_logits,\n",
    "                    {input_data: source_batch,\n",
    "                     target_sequence_length: targets_lengths,\n",
    "                     keep_prob: 1.0})\n",
    "\n",
    "                batch_valid_logits = sess.run(\n",
    "                    inference_logits,\n",
    "                    {input_data: valid_sources_batch,\n",
    "                     target_sequence_length: valid_targets_lengths,\n",
    "                     keep_prob: 1.0})\n",
    "\n",
    "                train_acc = get_accuracy(target_batch, batch_train_logits)\n",
    "                valid_acc = get_accuracy(valid_targets_batch, batch_valid_logits)\n",
    "    \n",
    "                print('Epoch {:>3} Batch {:>4}/{} - Train Accuracy: {:>6.4f}, Validation Accuracy: {:>6.4f}, Loss: {:>6.4f}'\n",
    "                      .format(epoch_i+1, batch_i, len(source_int_text) // batch_size, train_acc, valid_acc, loss))\n",
    "\n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, save_path)\n",
    "    print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_params(params):\n",
    "    with open('params.p', 'wb') as out_file:\n",
    "        pickle.dump(params, out_file)\n",
    "\n",
    "\n",
    "def load_params():\n",
    "    with open('params.p', mode='rb') as in_file:\n",
    "        return pickle.load(in_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save parameters for checkpoint\n",
    "save_params(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "_, (source_vocab_to_int, target_vocab_to_int), (source_int_to_vocab, target_int_to_vocab) = load_preprocess()\n",
    "load_path = load_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/dev\n",
      "Input\n",
      "  Word Ids:      [158, 50, 46, 196, 114, 182, 69]\n",
      "  English Words: lemons are your least favorite fruit .\n",
      "\n",
      "Prediction\n",
      "  Word Ids:      [204, 114, 182, 190, 180, 89, 190, 180, 20, 69, 1]\n",
      "  Reconstruc. Words: their favorite fruit is the banana is the pear . <EOS>\n"
     ]
    }
   ],
   "source": [
    "def sentence_to_seq(sentence, vocab_to_int):\n",
    "    results = []\n",
    "    for word in sentence.split(\" \"):\n",
    "        if word in vocab_to_int:\n",
    "            results.append(vocab_to_int[word])\n",
    "        else:\n",
    "            results.append(vocab_to_int['<UNK>'])\n",
    "            \n",
    "    return results\n",
    "\n",
    "translate_sentence = 'lemons are your least favorite fruit .'\n",
    "\n",
    "translate_sentence = sentence_to_seq(translate_sentence, source_vocab_to_int)\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(load_path + '.meta')\n",
    "    loader.restore(sess, load_path)\n",
    "\n",
    "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
    "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
    "    target_sequence_length = loaded_graph.get_tensor_by_name('target_sequence_length:0')\n",
    "    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "\n",
    "    translate_logits = sess.run(logits, {input_data: [translate_sentence]*batch_size,\n",
    "                                         target_sequence_length: [len(translate_sentence)*2]*batch_size,\n",
    "                                         keep_prob: 1.0})[0]\n",
    "\n",
    "print('Input')\n",
    "print('  Word Ids:      {}'.format([i for i in translate_sentence]))\n",
    "print('  English Words: {}'.format(\" \".join([source_int_to_vocab[i] for i in translate_sentence])))\n",
    "\n",
    "print('\\nPrediction')\n",
    "print('  Word Ids:      {}'.format([i for i in translate_logits]))\n",
    "print('  Reconstruc. Words: {}'.format(\" \".join([target_int_to_vocab[i] for i in translate_logits])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
