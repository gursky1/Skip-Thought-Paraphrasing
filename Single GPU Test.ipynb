{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "import random\n",
    "import nltk\n",
    "\n",
    "# Testing for eager execution\n",
    "try:\n",
    "    tf.enable_eager_execution()\n",
    "except:\n",
    "    pass\n",
    "print('Tensorflow Version: ', tf.__version__)\n",
    "print('Using Eager Execution?: ', tf.executing_eagerly())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First lets make sure we are operating on GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    print('No GPU found. Please use a GPU to train your neural network.')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining our reading function for pulling in data\n",
    "def load_data(path):\n",
    "    with open(path, 'r', encoding='utf-8') as file:\n",
    "        data = file.read()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looks like we need to whitespace the punctuation\n",
    "def whitespace_punct(sent_list):\n",
    "    whitespaced = [re.sub('([.,!?;()\"])', r' \\1 ', x).strip() for x in sent_list]\n",
    "    whitespaced = [re.sub('\\s{2,}', ' ', x).strip() for x in whitespaced]\n",
    "    whitespaced = [x.replace('-',' - ') for x in whitespaced]\n",
    "    return whitespaced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining our data preprocessing function\n",
    "def preprocess_mobydick(path, max_length=None):\n",
    "    # Loading our raw text data\n",
    "    raw_data = load_data(path)\n",
    "    \n",
    "    # Dropping lines that have chapter in them\n",
    "    raw_data = '\\n'.join([x for x in raw_data.split('\\n') if not 'CHAPTER' in x])\n",
    "    \n",
    "    # Getting rid of some special characters\n",
    "    raw_data = raw_data.replace('\\ufeff','')\n",
    "    raw_data = raw_data.replace('\\n',' ')\n",
    "    raw_data = raw_data.replace('-',',')\n",
    "    raw_data = raw_data.replace('—',' , ')\n",
    "    raw_data = raw_data.replace('”','\"')\n",
    "    raw_data = raw_data.replace('“','\"')\n",
    "    raw_data = raw_data.replace('  ',' ')\n",
    "    raw_data = raw_data.replace('_','')\n",
    "    raw_data = raw_data.replace('?\"', '? \"')\n",
    "    raw_data = raw_data.replace('!\"', '! \"')\n",
    "    raw_data = raw_data.replace('.\"', '. \"')\n",
    "    raw_data = raw_data.replace('Oh!','Oh,')\n",
    "    raw_data = raw_data.replace('’',\"'\")\n",
    "    raw_data = raw_data.replace(';', ' . ')\n",
    "    \n",
    "    # Prepping our sentence tokenizer\n",
    "    sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    \n",
    "    # Setting exceptions to tokenization\n",
    "    extra_abbreviations = set(['Dr', 'Mr', 'Mrs', 'Prof', 'Ms'])\n",
    "    sentence_tokenizer._params.abbrev_types.update(extra_abbreviations)\n",
    "\n",
    "    # Tokenizing text\n",
    "    sentences = sentence_tokenizer.tokenize(raw_data)\n",
    "\n",
    "    # Rejoining\n",
    "    sentences = '\\n'.join(sentences).lower()\n",
    "    \n",
    "    # Getting rid of endnotes\n",
    "    sentences = sentences.split('end of the project gutenberg ebook')[0]\n",
    "\n",
    "    # Splitting back up\n",
    "    sentences = sentences.split('\\n')\n",
    "    \n",
    "    # Whitespacing\n",
    "    sentences = whitespace_punct(sentences)\n",
    "    \n",
    "    # Removing empty sentences\n",
    "    sentences = [x for x in sentences if len(x)>0]\n",
    "    \n",
    "    # Cutting down long sentences\n",
    "    if max_length is not None:\n",
    "        sentences = [x if len(x.split())<max_length else ' '.join(x.split()[0:max_length]) for x in sentences]\n",
    "    \n",
    "    # Joining sentences\n",
    "    sentences = '\\n'.join(sentences)\n",
    "    \n",
    "    # Replacing titles\n",
    "    sentences = sentences.replace('mr . ','mr. ').replace('ms . ','ms. ').replace('mrs . ', 'mrs. ').replace('--',' : ')\n",
    "    \n",
    "    # Resplitting sentences\n",
    "    sentences = sentences.split('\\n')\n",
    "    \n",
    "    # Making sure trailing, leading whitespaces removed\n",
    "    sentences = [x.lower().rstrip().strip() for x in sentences]\n",
    "    \n",
    "    # Adding our start and stop tokens\n",
    "    sentences = ['<start> ' + x + ' <end>' for x in sentences]\n",
    "    \n",
    "    # Returning sentences\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading text\n",
    "sents = preprocess_mobydick('MobyDick.txt', max_length=30)\n",
    "\n",
    "# How many sentences in our corpus?\n",
    "print('Number of sentences in corpus: ',len(sents))\n",
    "\n",
    "# How many distinct tokens?\n",
    "print('Number of tokens: ', len(set('\\n'.join(sents).split(' '))))\n",
    "\n",
    "# What is the max sentence length?\n",
    "print('Max sentence length: ', max([len(x.split()) for x in sents]))\n",
    "\n",
    "# What does our dataset look like?\n",
    "print(*sents, sep = '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining our tokenizer\n",
    "def tokenize(lang, vocab_size):\n",
    "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', num_words=vocab_size)\n",
    "    \n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    "    \n",
    "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "    \n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "  \n",
    "    return tensor, lang_tokenizer\n",
    "# Some code adapted from\n",
    "# https://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/sequences/nmt_with_attention.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating our dataset function\n",
    "def load_dataset(path, max_len=None, vocab_size=None):\n",
    "    # creating cleaned input, output pairs\n",
    "    corpus = preprocess_mobydick(path, max_len)\n",
    "    \n",
    "    # Tokenizing our text\n",
    "    corpus, corpus_tokenizer = tokenize(corpus, vocab_size)\n",
    "    \n",
    "    return corpus, corpus_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining our tensor length function\n",
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)\n",
    "# Some code adapted from\n",
    "# https://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/sequences/nmt_with_attention.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading our data\n",
    "corpus, corpus_tokenizer = load_dataset('MobyDick.txt', max_len=30, vocab_size=1000)\n",
    "\n",
    "# Calculate max_length of the target tensors\n",
    "max_length_corpus = max_length(corpus)\n",
    "\n",
    "# Printing the data's shape\n",
    "print('Tokenized Corpus Shape: ', corpus.shape)\n",
    "\n",
    "# Printing the first 3 lines\n",
    "print(' First 3 lines of data:\\n',corpus[1:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating our skip-thought data creator\n",
    "def skip_thought_data(corpus):\n",
    "    # Creating our pre and post data\n",
    "    pre_corpus, post_corpus = corpus[:-2], corpus[2:]\n",
    "    \n",
    "    # Removing 1st and 2nd obs from corpus\n",
    "    corpus = corpus[1:-1]\n",
    "    \n",
    "    return corpus, pre_corpus, post_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus, pre_corpus, post_corpus = skip_thought_data(corpus)\n",
    "print('Encoding shape: ', corpus.shape)\n",
    "print('Previous shape: ', pre_corpus.shape)\n",
    "print('Post shape: ', post_corpus.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making sure it works\n",
    "print('Previous line: ', pre_corpus[0][0:4])\n",
    "print('Current line: ', corpus[0][0:4])\n",
    "print('Next line: ', post_corpus[0][0:4])\n",
    "# Looks like it works great!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the buffer size\n",
    "BUFFER_SIZE = 1024\n",
    "\n",
    "# Setting our batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Number of epochs to train over\n",
    "EPOCHS = 5\n",
    "\n",
    "# Number of rounds with no improvement to stop after\n",
    "early_stopping_rounds = 5\n",
    "\n",
    "# How many steps do we need to take per epoch?\n",
    "steps_per_epoch = len(corpus)//BATCH_SIZE\n",
    "\n",
    "# The dimension of our word embeddings\n",
    "embedding_dim = 256\n",
    "\n",
    "# The number of RNN cells to include in the recurrent layer\n",
    "units = 256\n",
    "\n",
    "# The dropout rate of the recurrent cells to help generalize\n",
    "dropout = 0\n",
    "\n",
    "# Determine the clipping threshold for our gradients to ease training\n",
    "gradient_clip = 1\n",
    "\n",
    "# Define the learning rate of our optimizer\n",
    "learning_rate = 0.005\n",
    "\n",
    "# Defining the momentum\n",
    "moment = 0.9\n",
    "\n",
    "# Setting vocab sizes\n",
    "vocab_size = len(corpus_tokenizer.word_index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining our encoder\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units, \n",
    "                                       return_sequences=True, \n",
    "                                       return_state=True, \n",
    "                                       recurrent_initializer='he_uniform')\n",
    "        self.drop = tf.keras.layers.Dropout(rate=dropout)\n",
    "        \n",
    "    def call(self, x, hidden, training=False):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state = hidden)     \n",
    "        if training:\n",
    "            output = self.drop(output, training)\n",
    "        return output, state\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))\n",
    "# Some code adapted from\n",
    "# https://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/sequences/nmt_with_attention.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining our Decoder\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz, dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units, \n",
    "                                       return_sequences=True, \n",
    "                                       return_state=True, \n",
    "                                       recurrent_initializer='he_uniform')\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        self.drop = tf.keras.layers.Dropout(rate=dropout)\n",
    "\n",
    "    def call(self, x, hidden, enc_output, training=False):\n",
    "        # Calling our first GRU layer\n",
    "        output, state = self.gru(enc_output, hidden)\n",
    "        \n",
    "        # Applying dropout\n",
    "        if training:\n",
    "            output = self.drop(output, training)\n",
    "            \n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (output.shape[0], -1))\n",
    "\n",
    "        # output shape == (batch_size, vocab)\n",
    "        output = self.fc(output)\n",
    "\n",
    "        return output, state\n",
    "    \n",
    "# Some code adapted from\n",
    "# https://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/sequences/nmt_with_attention.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining our loss function\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    \n",
    "    return tf.reduce_mean(loss_)\n",
    "# Some code adapted from\n",
    "# https://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/sequences/nmt_with_attention.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining our NMT loss function\n",
    "@tf.function\n",
    "def nmt_train_step(inp, out, enc_hidden):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        enc_output, enc_hidden = nmt_encoder(inp, enc_hidden, training=True)    \n",
    "\n",
    "        # Current sentence pass\n",
    "        dec_hidden = enc_hidden\n",
    "        dec_input = tf.expand_dims([corpus_tokenizer.word_index['<start>']] * BATCH_SIZE, 1)   \n",
    "        for t in range(1, pre.shape[1]):\n",
    "            predictions, dec_hidden = nmt_decoder(dec_input, dec_hidden, enc_output, training=True)\n",
    "            loss += loss_function(out[:, t], predictions)\n",
    "            dec_input = tf.expand_dims(out[:, t], 1)\n",
    "\n",
    "    batch_loss = (loss / int(out.shape[1]))\n",
    "\n",
    "    variables = nmt_encoder.trainable_variables + nmt_decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    \n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "  \n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining our training step for pre and post data for skip_thoughts\n",
    "@tf.function\n",
    "def skip_thoughts_train_step(inp, pre, post, enc_hidden):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        enc_output, enc_hidden = st_encoder(inp, enc_hidden, training=True)    \n",
    "\n",
    "        # Preceeding sentence pass\n",
    "        dec_hidden = enc_hidden\n",
    "        dec_input = tf.expand_dims([corpus_tokenizer.word_index['<start>']] * BATCH_SIZE, 1)   \n",
    "        for t in range(1, pre.shape[1]):\n",
    "            predictions, dec_hidden = st_pre_decoder(dec_input, dec_hidden, enc_output, training=True)\n",
    "            loss += loss_function(pre[:, t], predictions)\n",
    "            dec_input = tf.expand_dims(pre[:, t], 1)\n",
    "        \n",
    "        # Following sentence pass\n",
    "        dec_hidden = enc_hidden\n",
    "        dec_input = tf.expand_dims([corpus_tokenizer.word_index['<start>']] * BATCH_SIZE, 1)   \n",
    "        for t in range(1, post.shape[1]):\n",
    "            predictions, dec_hidden = st_post_decoder(dec_input, dec_hidden, enc_output, training=True)\n",
    "            loss += loss_function(post[:, t], predictions)\n",
    "            dec_input = tf.expand_dims(post[:, t], 1) \n",
    "\n",
    "    batch_loss = (loss / int(pre.shape[1]))\n",
    "\n",
    "    variables = st_encoder.trainable_variables + st_pre_decoder.trainable_variables + st_post_decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    \n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "  \n",
    "    return batch_loss\n",
    "# Some code adapted from\n",
    "# https://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/sequences/nmt_with_attention.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up our NMT Model\n",
    "# Creating our dataset\n",
    "nmt_dataset = tf.data.Dataset.from_tensor_slices((corpus, corpus)).shuffle(BUFFER_SIZE)\n",
    "nmt_dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "# Defining encoder\n",
    "nmt_encoder = Encoder(vocab_size, embedding_dim, units, BATCH_SIZE, dropout)\n",
    "# Defining decoder\n",
    "nmt_decoder = Decoder(vocab_size, embedding_dim, units, BATCH_SIZE, dropout)\n",
    "# Defining optimizer\n",
    "nmt_optimizer = tf.keras.optimizers.SGD(lr=learning_rate, momentum = moment, clipvalue=gradient_clip)\n",
    "# Defining loss function\n",
    "nmt_loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "# Defining our checkpoint\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt/nmt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=nmt_optimizer,\n",
    "                                 encoder=nmt_encoder,\n",
    "                                 decoder=nmt_decoder)\n",
    "# Some code adapted from\n",
    "# https://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/sequences/nmt_with_attention.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training our NMT Model\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    enc_hidden = st_encoder.initialize_hidden_state()\n",
    "    loss = 0\n",
    "    \n",
    "    # Calculation loss and applying gradients on training batches\n",
    "    for (batch, (sent, out)) in enumerate(st_dataset.take(steps_per_epoch)):\n",
    "        batch_loss = nmt_train_step(sent, out, enc_hidden)\n",
    "        print('Batch {}/{} - {:.4f}'.format(batch, steps_per_epoch, batch_loss))\n",
    "        loss += batch_loss\n",
    "        \n",
    "    # Creating our meaned losses\n",
    "    loss = loss/steps_per_epoch\n",
    "    \n",
    "    # Printing out our progress\n",
    "    print('NMT: Epoch = {} | Training Loss = {:.4f} | Train Time = {:.2f} sec\\n'.format(epoch + 1,\n",
    "                                                                                       loss,\n",
    "                                                                                       time.time() - start))\n",
    "checkpoint.save(file_prefix = checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up our Skip-thoughts model\n",
    "# Creating our dataset\n",
    "st_dataset = tf.data.Dataset.from_tensor_slices((corpus, pre_corpus, post_corpus)).shuffle(BUFFER_SIZE)\n",
    "st_dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "# Defining encoder\n",
    "st_encoder = Encoder(vocab_size, embedding_dim, units, BATCH_SIZE, dropout)\n",
    "# Defining decoder\n",
    "st_pre_decoder = Decoder(vocab_size, embedding_dim, units, BATCH_SIZE, dropout)\n",
    "st_post_decoder = Decoder(vocab_size, embedding_dim, units, BATCH_SIZE, dropout)\n",
    "# Defining optimizer\n",
    "st_optimizer = tf.keras.optimizers.SGD(lr=learning_rate, momentum = moment, clipvalue=gradient_clip)\n",
    "# Defining loss function\n",
    "st_loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "# Defining our checkpoint\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt/skip-thoughts\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=st_optimizer,\n",
    "                                 encoder=st_encoder,\n",
    "                                 pre_decoder = st_pre_decoder\n",
    "                                 post_decoder=st_post_decoder)\n",
    "# Some code adapted from\n",
    "# https://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/sequences/nmt_with_attention.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training our skip-thoughts model\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    enc_hidden = st_encoder.initialize_hidden_state()\n",
    "    loss = 0\n",
    "    \n",
    "    # Calculation loss and applying gradients on training batches\n",
    "    for (batch, (sent, pre, post)) in enumerate(st_dataset.take(steps_per_epoch)):\n",
    "        batch_loss = skip_thoughts_train_step(sent, pre, post, enc_hidden)\n",
    "        print('Batch {}/{} - {:.4f}'.format(batch, steps_per_epoch, batch_loss))\n",
    "        loss += batch_loss\n",
    "        \n",
    "    # Creating our meaned losses\n",
    "    loss = loss/steps_per_epoch\n",
    "    \n",
    "    # Printing out our progress\n",
    "    print('Skip-thoughts: Epoch = {} | Training Loss = {:.4f} | Train Time = {:.2f} sec\\n'.format(epoch + 1,\n",
    "                                                                                                  loss,\n",
    "                                                                                                  time.time() - start))\n",
    "checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "# Some code adapted from\n",
    "# https://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/sequences/nmt_with_attention.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to reimport our models to get an estimate on cosine similarity to compare\n",
    "# We can use T-SNE or PCA to get down to 2 dimensions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
